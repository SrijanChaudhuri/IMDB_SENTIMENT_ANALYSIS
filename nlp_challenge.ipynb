{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## LIBRARIES\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport regex as re\nfrom string import punctuation\nimport math\n\nimport nltk\n#nltk.download(\"omw-1.4\")\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-27T12:25:10.803837Z","iopub.execute_input":"2022-12-27T12:25:10.804362Z","iopub.status.idle":"2022-12-27T12:25:32.477417Z","shell.execute_reply.started":"2022-12-27T12:25:10.804231Z","shell.execute_reply":"2022-12-27T12:25:32.476051Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"[nltk_data] Error loading omw-1.4: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-27T12:25:48.769055Z","iopub.execute_input":"2022-12-27T12:25:48.770272Z","iopub.status.idle":"2022-12-27T12:25:49.479519Z","shell.execute_reply.started":"2022-12-27T12:25:48.770209Z","shell.execute_reply":"2022-12-27T12:25:49.478319Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## EDA\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T12:29:37.529520Z","iopub.execute_input":"2022-12-27T12:29:37.530410Z","iopub.status.idle":"2022-12-27T12:29:37.541646Z","shell.execute_reply.started":"2022-12-27T12:29:37.530366Z","shell.execute_reply":"2022-12-27T12:29:37.540447Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T12:26:07.801643Z","iopub.execute_input":"2022-12-27T12:26:07.802118Z","iopub.status.idle":"2022-12-27T12:26:07.912777Z","shell.execute_reply.started":"2022-12-27T12:26:07.802075Z","shell.execute_reply":"2022-12-27T12:26:07.911446Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                   review sentiment\ncount                                               50000     50000\nunique                                              49582         2\ntop     Loved today's show!!! It was a variety and not...  positive\nfreq                                                    5     25000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50000</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>49582</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Loved today's show!!! It was a variety and not...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>25000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.drop_duplicates(subset='review', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T07:05:05.789643Z","iopub.execute_input":"2022-10-01T07:05:05.790243Z","iopub.status.idle":"2022-10-01T07:05:05.954582Z","shell.execute_reply.started":"2022-10-01T07:05:05.790209Z","shell.execute_reply":"2022-10-01T07:05:05.953425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-10-01T07:05:05.956022Z","iopub.execute_input":"2022-10-01T07:05:05.956378Z","iopub.status.idle":"2022-10-01T07:05:05.999452Z","shell.execute_reply.started":"2022-10-01T07:05:05.95634Z","shell.execute_reply":"2022-10-01T07:05:05.998537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['review'] = data['review'].str.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CLEANING\n\ndef punctuation_remove(text):\n    exclude=string.punctuation\n    for char in exclude:\n        text=text.replace(char,'')\n    return text\n\ndata['review'] = data['review'].apply(punctuation_remove)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T07:05:06.003747Z","iopub.execute_input":"2022-10-01T07:05:06.004034Z","iopub.status.idle":"2022-10-01T07:05:09.086124Z","shell.execute_reply.started":"2022-10-01T07:05:06.004008Z","shell.execute_reply":"2022-10-01T07:05:09.085068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tag_remove(text):\n    pattern=re.compile('<.*?>')\n    return pattern.sub(r'',text)\n\ndata['review']=data['review'].apply(tag_remove)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T07:05:09.087487Z","iopub.execute_input":"2022-10-01T07:05:09.087934Z","iopub.status.idle":"2022-10-01T07:05:09.100698Z","shell.execute_reply.started":"2022-10-01T07:05:09.087896Z","shell.execute_reply":"2022-10-01T07:05:09.099581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def url_remove(text):\n    pattern=re.compile(r'https ? ://\\s+|www\\.\\s+')\n    return pattern.sub(r'',text)\ndata['review']=data['review'].apply(url_remove)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stopwords_remove(text):\n    return ' '.join([x.lower() for x in text.split(' ') if x.lower() not in STOPWORDS])\n\ndata['review'] = data['review'].apply(remove_stop)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T07:05:09.102522Z","iopub.execute_input":"2022-10-01T07:05:09.103192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['review']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(series):\n    return word_tokenize(series)\n\ndata['tokens'] = data['review'].apply(tokenize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_len(series):\n    return len(series)\n\ndata['token_len'] = data['tokens'].apply(get_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['tokens','token_len']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PADDING\n\nMAX_LEN = math.ceil(data.describe().values[1])\nprint(MAX_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_token(series):\n    if len(series) < MAX_LEN:\n        series.extend(['<END>']*(MAX_LEN-len(series)))\n        return series\n    else:\n        return series[:MAX_LEN]\n\ndata['paded_tokens'] = data['tokens'].apply(pad_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data['paded_tokens'].values[10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['tokens','paded_tokens']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NORMALIZATION (LEMMATIZATION)\n\nlemmatizer = WordNetLemmatizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemma(series):\n    return [lemmatizer.lemmatize(word) for word in series]\n\ndata['lemma_tokens'] = data['paded_tokens'].apply(lemma)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['tokens','lemma_tokens']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NORMALIZATION (STEMMING)\n\nstemmer = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stem(series):\n    return [stemmer.stem(word) for word in series]\n\ndata['stem_tokens'] = data['tokens'].apply(stem)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['tokens','stem_tokens']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# POS TAGGING\n\ndef pos_t(series):\n    return nltk.pos_tag(series, tagset='universal')\n\ndata['pos_tag_tokens'] = data['tokens'].apply(pos_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['tokens','pos_tag_tokens']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WORD EMBEDDINGS\n\nunique_words = set()\nfor tokens in list(data['lemma_tokens'].values):\n    unique_words.update(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Count of Unique words:', len(unique_words))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2idx = {}\nfor word in unique_words:\n    word2idx[word] = len(word2idx)\nword2idx['<END>'] = len(word2idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_embeddings = np.random.rand(len(word2idx),200)\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt', 'r') as embeds:\n    embeddings = embeds.read()\n    embeddings = embeddings.split('\\n')[:-2]\n    \nfor token_idx, token_embed in enumerate(embeddings):\n    token = token_embed.split()[0]\n    if token in word2idx:\n        word_embeddings[word2idx[token]] = [float(val) for val in token_embed.split()[1:]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Word embeddings for word {list(word2idx.keys())[300]}:',word_embeddings[300])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COUNT VECTORIZATION\n\nvectorizer = CountVectorizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vector_cl = vectorizer.fit_transform([' '.join(tok) for tok in list(data['lemma_tokens'].values)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vector_cl.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VECTORIZING THE LABELS\n\ndef label_vec(series):\n    return int(series=='positive')\n\ndata['sentiment'] = data['sentiment'].apply(label_vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['lemma_tokens', 'sentiment']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAINING MODEL\n\nvector_cl_train_x, vector_cl_test_x, vector_cl_train_y, vector_cl_test_y = train_test_split(vector_cl, data['sentiment'].values, test_size=0.3, random_state=42)\nprint(vector_cl_train_x.shape,vector_cl_train_y.shape,vector_cl_test_x.shape,vector_cl_test_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FITTING MODEL\n\nlog_reg = LogisticRegression().fit(vector_cl_train_x, vector_cl_train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = log_reg.predict(vector_cl_test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean Accuracy:', log_reg.score(vector_cl_test_x, vector_cl_test_y))\nprint('F1 Score:', f1_score(vector_cl_test_y, pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TESTING\n\nprint(log_reg.predict(vectorizer.transform(['This movie was one of the best i watched in recent times'])))\nprint(log_reg.predict(vectorizer.transform(['This movie was not bad and i really liked it.'])))\nprint(log_reg.predict(vectorizer.transform(['The cinematics of this movie made my eyes bleed'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The inability of these classical machine learning models to identify the context of the text is the reason that Recurrent neural networks and transformer based models are the most widely used models for such tasks as they tend to understand the context of the text. \n\nBelow we will be training a hugging face model.","metadata":{}},{"cell_type":"code","source":"# A HUGGING FACE MODEL APPROACH\n\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nimdb = load_dataset(\"imdb\")\n\ntrain_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(10000))])\ntest_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(10000))])\n\ncheckpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ndef preprocess_function(examples):\n   return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=132)\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n\ntokenized_train = tokenized_train.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"label\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=32,\n)\n\ntokenized_test = tokenized_test.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"label\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=32,\n)\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\ndef compute_metrics(eval_pred):\n   load_accuracy = load_metric(\"accuracy\")\n   load_f1 = load_metric(\"f1\")\n  \n   logits, labels = eval_pred\n   predictions = np.argmax(logits, axis=-1)\n   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n   return {\"accuracy\": accuracy, \"f1\": f1}\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0001),\n    loss=SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    tokenized_train,\n    validation_data=tokenized_test,\n    epochs=3\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TextClassificationPipeline\nsentiment = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework='tf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment(['This movie was one of the best i watched in recent times','This movie was not bad and i really liked it.','The cinematics of this movie made my eyes bleed'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}